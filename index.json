[{"content":"Thank you very much for reading my blog.\nMy name is MD Sayem Ahmed. I am a Software Engineer by profession and currently living and working in Berlin, Germany.\nI love working with complex distributed systems and building high-performance web applications. I am most experienced in Java-based technologies like Spring, JPA, Servlets, JSP, and related technologies, but I don\u0026rsquo;t mind working with other languages and tools if needed.\nHere is a short list of my online profiles where you can find more information about me and my work -\nLinkedIn StackOverflow Github DZone JavaCodeGeeks Scrum Alliance Drop me an email here if you want to say hi :-) .\nDisclaimer: The views and opinions expressed here are my own only and in no way represent the views, positions or opinions – expressed or implied – of my employer (present and past).\n","permalink":"https://sayembd.github.io/about-me/","summary":"Thank you very much for reading my blog.\nMy name is MD Sayem Ahmed. I am a Software Engineer by profession and currently living and working in Berlin, Germany.\nI love working with complex distributed systems and building high-performance web applications. I am most experienced in Java-based technologies like Spring, JPA, Servlets, JSP, and related technologies, but I don\u0026rsquo;t mind working with other languages and tools if needed.\nHere is a short list of my online profiles where you can find more information about me and my work -","title":"About Me"},{"content":"Introduction Recently I have been hearing about observability more and more. The promise that an observable system can help us to debug and identify performance and reliability issues in a microservice architecture sounded quite good to me. Hence I decided to read up and learn more on this topic. In this blog post I will try to summarise what I have learned about observability so far.\nWhat is Observability? Observability is a term or a concept that has its root in Physics, mainly in Control Theory. According to Wikipedia -\nObservability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs.\n\u0026hellip; A system is said to be observable if, for any possible evolution of state and control vectors, the current state can be estimated using only the information from outputs (physically, this generally corresponds to information obtained by sensors). In other words, one can determine the behavior of the entire system from the system\u0026rsquo;s outputs. On the other hand, if the system is not observable, there are state trajectories that are not distinguishable by only measuring the outputs.\nExtending the same concept to a software system, we can say -\nA software system is observable if we can ask new questions from the outside to understand what is going on on the inside, all without deploying new code.\nSo in effect, observability is a measure of how well we can make sense of what is going on with our application by asking arbitrary questions (the unknown-unknowns) about the system, without having to know the questions in advance. The more observable our systems are, the more arbitrary questions we are able to ask. Used effectively, it can greatly improve our application quality and reliability by making it relatively easy to debug and identify potential performance and reliability issues in production.\nWhy should I care about Observability? Because software is becoming more and more complex than what it used to be, making it more difficult to predict most of the production issues in advance.\nConsider a regular monolithic application. In such applications the entire codebase is in one place, making it possible to browse through different use cases end-to-end and anticipate most (if not all) of the production issues in advance. Once the problematic areas have been identified we augment the application code to collect and report various metrics, visualise these metrics in dashboards, and create alerts. Combining application logs with with these collected metrics was often enough to debug most of the performance and reliability issues in production. If we could also throw in distributed tracing to the mix, then our chances of finding and quickly fixing these issues would increase even further.\nContrast this with the current trend in the software world. Nowadays we see a clear preference among companies to decompose large monolithic applications into smaller-sized microservices in order to achieve greater business agility. As a result, systems are becoming more and more distributed. Small and independent teams are working on different distributed systems in parallel whose code bases are separate. What used to be functions invocations before have now been converted to network calls between remote applications. With the adoption of DevOps practices releases are becoming more frequent, reducing the time needed to release features to production once they are ready. All of these are resulting in more moving parts in a system which are changing frequently at their own pace, making it difficult to predict how an application will behave in production. Often times, questions like “will the release of this new features in application X interfere with the existing features in application a? What about application b? Or c?….” can be answered only after we release application X in production. As a direct consequence of adopting a microservice-oriented architecture, debugging gets more difficult than a monolithic system.\nThis is where the concept of observability is particularly useful. Being able to ask arbitrary questions about our entire system without having to know them in advance can greatly reduce the burden of debugging and identifying performance and reliability issues in a microservice architecture.\nThe 3 pillars of Observability Metrics, Logs, and Distributed Traces are often called the 3 pillars of observability because these are the tools we have been traditionally using to make sense of our system. Most of us already familiar with these concepts and related tools, so we are not going to dive deep into them in this article. Instead, we will try to understand why these 3 pillars are not enough to create an observable system.\nMetrics Metrics are numerical measurements taken over intervals of time. We use metrics to to measure response times of requests, to count the number requests that failed to get a valid response etc. For a long time metrics have been the standard way to monitor the overall health of an application - number of live instances, current memory consumption, cpu usage, response times, query execution time etc. They are also being used to trigger alerts in case of emergencies like instances going down, low memory, high cpu usage etc. All in all, very useful tool.\nHowever, traditional metrics-based tools are not enough to create an observable system. One of the primary reasons for this is that any tools that are metric-based can deal with only low-cardinality dimensions. Things like user ids, purchase ids, shopping cart ids - any data that have high-cardinality are not collected with these tools as otherwise the cost would blow up. Also, in order to keep the associated costs low, metrics are aggregated on the client-side and then stored in their aggregated form, losing granularity even further. Without high-cardinality data it is difficult to investigate and debug issues in a microservice architecture. As a result any questions that are answered by a metric-based tool have to be pre-defined so that we can collect targeted metrics to answer them. This is an antithesis to the premise of observability as observability requires being able to ask arbitrary questions about a system without knowing them in advance. Without high-cardinality data, this is not possible.\nAnother downside is that the metrics that are collected are not tied to their source request which triggered them. In a microsevice-oriented architecture a single user request can hit many different services, query different databases or caches, send messages to queues or kafka topics, or can interact with any combination of these. We can collect metrics from each of these sources, but once collected we can never link them back together. This makes it difficult to answer questions like why do this particular group of users see a high response times of 10 seconds while our metrics dashboard is showing a p99 of 1 seconds?\nLogs Logs are a useful tool which help us debug issues by providing us context-dependent messages and stack traces. However, they cannot be used effectively to create an observable system.\nOne of the primary downsides of using logging to create an observable system is the associated cost. Systems that use logging to improve observability becomes too expensive to maintain. This is how Ben Sigelman, co-founder of Lightstep, explains the problem in one of his articles written on the Lightstep blog (I highly recommend to give the entire article a thorough read) -\nIf we want to use logs to account for individual transactions (like we used to in the days of a monolithic web server\u0026rsquo;s request logs), we would need to pay for the following:\nApplication transaction rate * all microservices * cost of network and storage * weeks of data retention = way, way too much $$$$ Logging systems can\u0026rsquo;t afford to store data about every transaction anymore because the cost of those transactional logs is proportional to the number of microservices touched by an average transaction.\nAnother downside is that In order to answer any arbitrary questions about our system we would have to log quite aggressively. Since traditional logging libraries cannot dynamically sample logs, logging excessively could adversely affect the performance of the application as a whole.\nDistributed Tracing This is how OpenTracing, a Cloud Native Computing Foundation project, defines Distributed Tracing -\nDistributed tracing, also called distributed request tracing, is a method used to profile and monitor applications, especially those built using a microservices architecture. Distributed tracing helps pinpoint where failures occur and what causes poor performance.\nDistributed Tracing has its use in building an observable system. After all, they are the threads with which we can connect an end-to-end request in a microservice architecture. However, they come with a few challenges on their own.\nThe first challenge is choosing a right sampling strategy. Traditionally distributed tracing tools have been making the decision of whether to sample a request or not at the very beginning, when a request enters the infrastructure for the first time from outside. This results in a sampling strategy that is either too aggressive and collect too much data which are expensive to store and analyse, or too relaxed and does not collect enough data to help us with observability.\nThe second challenge is the UI with which we analyse the trace data. Tracing tools usually come with a UI component which display all the traces in what is called a trace view. In a system with hundreds of services where a typical request touches 20 or 30 of them, the trace view becomes too complex for a human to analyse without any automated support. In addition, spans, which are treated as units of work by the tracing systems and responsible for capturing trace data from services, are too low level to be used for debugging purposes. Cindy Sridharan wrote an excellent article on this topic where she explains the problem in a much better way -\nAdmittedly, some tracing systems provide condensed traceviews when the number of spans in a trace are so exceedingly large that they cannot be displayed in a single visualization. Yet, the amount of information being encapsulated even in such pared down views still squarely puts the onus on the engineers to sift through all the data the traceview exposes and narrow down the set of culprit services. This is an endeavor machines are truly faster, more repeatable and less error-prone than humans at accomplishing.\n\u0026hellip; The fundamental problem with the traceview is that a span is too low-level a primitive for both latency and “root cause” analysis. It’s akin to looking at individual CPU instructions to debug an exception when a much higher level entity like a backtrace would benefit day-to-day engineers the most.\nFurthermore, I’d argue that what is ideally required isn’t the entire picture of what happened during the lifecycle of a request that modern day traces depict. What is instead required is some form of higher level abstraction of what went wrong (analogous to the backtrace) along with some context. Instead of seeing an entire trace, what I really want to be seeing is a portion of the trace where something interesting or unusual is happening. Currently, this process is entirely manual: given a trace, an engineer is required to find relevant spans to spot anything interesting. Humans eyeballing spans in individual traces in the hopes of finding suspicious behavior simply isn’t scalable, especially when they have to deal with the cognitive overhead of making sense of all the metadata encoded in all the various spans like the span ID, the RPC method name, the duration of the span, logs, tags and so forth.\nI highly recommend you to give the article a thorough read.\nWhat does an ideal Observability tool look like? Charity Majors, co-founder of HoneyComb, wrote an excellent article on the HoneyComb blog where she mentions the criteria that a tool must fulfil in order to deliver observability -\nArbitrarily-wide structured raw events Context persisted through the execution path Without indexes or schemas High-cardinality, high-dimensionality Ordered dimensions for traceability Client-side dynamic sampling An exploratory visual interface that lets you slice and dice and combine dimensions In close to real-time She then goes on to explain the reasoning behind her choices, all of which I fully agree with. I highly recommend giving the article a thorough read.\nTrying out an existing Observability tool - HoneyComb In the same article that I have mentioned just now, Charity mentions how HoneyComb was built to deliver on these promises. Hence I decided to give it a try by checking out their live play scenarios.\nIn the Play with Tracing and BubbleUp scenario I followed the step by step guide to identify some outlier requests which were taking longer than the rest. By the end of the demo I was able to nail the problem down to the individual user who was experiencing the slower response times. I could definitely see how this technique could help me to debug performance issues in production which are affecting a portion of the users but are not visible in my pre-defined metrics dashboard.\nNext I tried out the Play with Events scenario which contains data about an actual production incident that HoneyComb faced back in 2018. Using the step by step guide as before I was able to identify the failed database that was the root of the issue.\nI noticed the following aspects of the tool -\nHigh-cardinality data: In the first scenario I was able to link the response time with an individual user, and then link the slower response time with the individual query that was being executed. Without high-cardinality this would have been impossible. In the absence of high cardinality data I could at best try to guess the issue and add sporadic log statements here and there, but I would still have to rely on luck to give me a break. Debugging should not be tied to luck. Metrics are also tied to requests/traces: All response times were tied with each individual trace, thus making it easy to identify requests which were slow. Wide events: The trace events contained a lot of data, including even the database query that was executed by the affected user! Without this query it would have been difficult to nail it down to the database performance problem. Dynamic dashboards: all the dashboards that are being generated are fully dynamic, and it\u0026rsquo;s possible to create dashboards per dimension! At this point I was curious to know the strategy HoneyComb uses to decide which requests to sample. I searched in the doc and found the section about Dynamic Sampling, where it\u0026rsquo;s mentioned how it\u0026rsquo;s possible to make the sampling decision based on whether an HTTP request encounters an error -\nFor example: when recording HTTP events, we may care about seeing every server error but need less resolution when looking at successful requests. We can then set the sample rate for successful requests to 100 (storing one in a hundred successful events). We include the sample rate along with each event—100 for successful events and 1 for error events.\nHence with HoneyComb it is possible to delay the sampling decision once the request has been fully executed. This strategy is very handy and can be used to sample aggressively for failed/problematic requests and thus making it easy to debug them, while at the same time performing a relaxed sampling for the successful requests and thus helping us to keep the data volume low.\nOne other thing that I noticed - in order to identify which requests are slow, we first need to define what a slow request looks like. For some applications it may be perfectly acceptable if a request completes within 4 seconds, while for some other type of applications it might be too slow. Since SLI/SLO/SLAs are gaining more and more popularity in our industry, it would make sense to use SLOs to define these criteria, and then create sampling strategies based on this definition. If a request fails our SLO, we can always decide to sample and store the request so that we can later debug why it failed. If it is successful, we can adopt a more relaxed sampling rate.\nAll in all, HoneyComb has left quite a good impression on me. Indeed it\u0026rsquo;s an excellent tool!\nAre Metric-based monitoring tools going to be obsolete? I don\u0026rsquo;t think so. Metrics-based monitoring tools are still the best choice when we want to answer any pre-defined questions about our system (also called known-unknowns) -\nHow many application instances are live at the moment? What is the amount of memory being consumed by the applications? What is the CPU usage etc. Observability tools, on the other hand, are best at answering the unknown-unknowns, things like -\nWhy does this user sees a response time of 4 seconds? Why are the database queries hitting the database instances in region X take more than 5 seconds to complete etc. Any ideal observable system would combine them both.\nConclusion I am still at the very early stage of my observability journey, and still learning the concepts and the tools used in this field. However, I am already convinced that in a distributed system architecture observability practices are invaluable and can help us improve the quality and the reliability of our applications by a great deal. I intend to apply these practices in my day to day work and as I learn more I will definitely try to share my learnings in my blog (given time permits)!\nAcknowledgements These are the resources which helped me learned about what observability truly is and how to build an observable system -\nDefinition of Observability by Charity Majors Observability — A 3-Year Retrospective - Charity Majors Three Pillars with Zero Answers - Towards a New Scorecard for Observability - Ben Sigelman So You Want To Build An Observability Tool… - Charity Majors Distributed Systems Observability - Cindy Sridharan Distributed Tracing — we’ve been doing it wrong - Cindy Sridharan ","permalink":"https://sayembd.github.io/posts/how-to-build-observable-system-an-introduction-to-observability/","summary":"Introduction Recently I have been hearing about observability more and more. The promise that an observable system can help us to debug and identify performance and reliability issues in a microservice architecture sounded quite good to me. Hence I decided to read up and learn more on this topic. In this blog post I will try to summarise what I have learned about observability so far.\nWhat is Observability? Observability is a term or a concept that has its root in Physics, mainly in Control Theory.","title":"How to Build Observable Systems - An Introduction to Observability"},{"content":"Executable Specifications are tests that can also serve as design specifications. They enable technical and business teams to get on the same page by enabling the use of a common language (in DDD-world this is also known as Ubiquitous Language). They function as documentations for the future maintainers of the code. In this article we will see an opinionated way of writing automated tests which could also function as Executable Specifications.\nLet\u0026rsquo;s start with an example. Suppose we are creating an accounting system for a business. The system will allow its users to record incomes and expenses into different accounts. Before users can start recording incomes and expenses, they should be able to add new accounts into the system. Suppose that the specification for the \u0026ldquo;Add New Account\u0026rdquo; use case looks like below -\nScenario 1 Given account does not exist When user adds a new account Then added account has the given name Then added account has the given initial balance Then added account has user\u0026rsquo;s id\nScenario 2 Given account does not exist When user adds a new account with negative initial balance Then add new account fails\nScenario 3 Given account with the same name exists When user adds a new account Then add new account fails\nIn order to create a new account the user needs to enter an account name and an initial balance into the system. The system will then create the account if no account with the given name already exists and the given initial balance is positive.\nWe will first write down a test which will capture the first \u0026ldquo;Given-When-Then\u0026rdquo; part of the first scenario. This is how it looks like -\nclass AddNewAccountTest { @Test @DisplayName(\u0026#34;Given account does not exist When user adds a new account Then added account has the given name\u0026#34;) void accountAddedWithGivenName() { } } The @DisplayName annotation was introduced in JUnit 5. It assigns a human-readable name to a test. This is the label that we would see when we execute this test e.g., in an IDE like IntelliJ IDEA.\nWe will now create a class which will be responsible for adding the account -\nclass AddNewAccountService { void addNewAccount(String accountName) { } } The class defines a single method which accepts the name of an account and will be responsible for creating it i.e., saving it to a persistent data store. Since we decided to call this class AddNewAccountService, we will also rename our test to AddNewAccountServiceTest to follow the naming convention used in the JUnit world.\nWe can now proceed with writing our test -\nclass AddNewAccountServiceTest { @Test @DisplayName(\u0026#34;Given account does not exist When user adds a new account Then added account has the given name\u0026#34;) void accountAddedWithGivenName() { AddNewAccountService accountService = new AddNewAccountService(); accountService.addNewAccount(\u0026#34;test account\u0026#34;); // What to test? } } What should we test/verify to ensure that the scenario is properly implemented? If we read our specification again, it is clear that we want to create an \u0026ldquo;Account\u0026rdquo; with a user-given name, hence this is what we should try to test here. In order to do this, we will have to first create a class which will represent an Account -\n@AllArgsConstructor class Account { private String name; } The Account class has only one property called name. It will have other fields like user id and balance, but we are not testing those at the moment, hence we will not add them to the class right away.\nNow that we have created the Account class, how do we save it, and more importantly, how do we test that the account being saved has the user-given name? There are many approaches to do this, and my preferred one is to define an interface which will encapsulate this saving action. Let\u0026rsquo;s go ahead and create it -\ninterface SaveAccountPort { void saveAccount(Account account); } The AddNewAccountService will be injected with an implementation of this interface via constructor injection -\n@RequiredArgsConstructor class AddNewAccountService { private final SaveAccountPort saveAccountPort; void addNewAccount(String accountName) { } } For testing purposes we will create a mock implementation with the help of Mockito so that we don\u0026rsquo;t have to worry about the actual implementation details -\n@ExtendWith(MockitoExtension.class) class AddNewAccountServiceTest { @Mock private SaveAccountPort saveAccountPort; @Test @DisplayName(\u0026#34;Given account does not exist When user adds a new account Then added account has the given name\u0026#34;) void accountAddedWithGivenName() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); accountService.addNewAccount(\u0026#34;test account\u0026#34;); // What to test? } } Our test setup is now complete. We now expect our method under test, the addNewAccount method of the AddNewAccountService class, to invoke the saveAccount method of the SaveAccountPort, with an Account object whose name is set to the one passed to the method. Let\u0026rsquo;s codify this in our test -\n@ExtendWith(MockitoExtension.class) class AddNewAccountServiceTest { @Mock private SaveAccountPort saveAccountPort; @Captor private ArgumentCaptor\u0026lt;Account\u0026gt; accountArgumentCaptor; @Test @DisplayName(\u0026#34;Given account does not exist When user adds a new account Then added account has the given name\u0026#34;) void accountAddedWithGivenName() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); accountService.addNewAccount(\u0026#34;test account\u0026#34;); BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); BDDAssertions.then(accountArgumentCaptor.getValue().getName()).isEqualTo(\u0026#34;test account\u0026#34;); } } The line below -\nBDDMockito.then(saveAccountPort) .should() .saveAccount(accountArgumentCaptor.capture()); verifies that the saveAccount method of the SaveAccountPort is invoked once the method under test is invoked. We also capture the account argument that is passed to the saveAccount method with our argument captor. The next line -\nBDDAssertions.then(accountArgumentCaptor.getValue().getName()) .isEqualTo(\u0026#34;test account\u0026#34;); then verifies that the captured account argument has the same name as the one that was passed in the test.\nIn order to make this test pass, the minimal code that is needed in our method under test is as follows -\n@RequiredArgsConstructor class AddNewAccountService { private final SaveAccountPort saveAccountPort; void addNewAccount(String accountName) { saveAccountPort.saveAccount(new Account(accountName)); } } With that, our test starts to pass!\nLet\u0026rsquo;s move on to the second \u0026ldquo;Then\u0026rdquo; part of the first scenario, which says -\nThen added account has the given initial balance Let\u0026rsquo;s write another test which will verify this part -\n@Test @DisplayName(\u0026#34;Given account does not exist When user adds a new account Then added account has the given initial balance\u0026#34;) void accountAddedWithGivenInitialBalance() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); accountService.addNewAccount(\u0026#34;test account\u0026#34;, \u0026#34;56.0\u0026#34;); BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); BDDAssertions.then(accountArgumentCaptor.getValue().getBalance()) .isEqualTo(new BigDecimal(\u0026#34;56.0\u0026#34;)); } We have modified our addNewAccount method to accept the initial balance as the second argument. We have also added a new field, called balance, in our Account object which is able to store the account balance -\n@AllArgsConstructor @Getter class Account { private String name; private BigDecimal balance; } Since we have changed the signature of the addNewAccount method, we will also have to modify our first test -\n@Test @DisplayName(\u0026#34;Given account does not exist When user adds a new account Then added account has the given name\u0026#34;) void accountAddedWithGivenName() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); accountService.addNewAccount(\u0026#34;test account\u0026#34;, \u0026#34;1\u0026#34;); BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); BDDAssertions.then(accountArgumentCaptor.getValue().getName()).isEqualTo(\u0026#34;test account\u0026#34;); } If we run our new test now it will fail as we haven\u0026rsquo;t implemented the functionality yet. Let\u0026rsquo;s do that now -\nvoid addNewAccount(String accountName, String initialBalance) { saveAccountPort.saveAccount(new Account(accountName, new BigDecimal(initialBalance))); } Both of our tests should pass now.\nAs we already have a couple of tests in place, it\u0026rsquo;s time to take a look at our implementation and see if we can make it better. Since our AddNewAccountService is as simple as it can be, we don\u0026rsquo;t have to do anything there. As for our tests, we could eliminate the duplication in our test setup code - both tests are instantiating an instance of the AddNewAccountService and invoking the addNewAccount method on it in the same way. Whether to remove or keep this duplication depends on our style of writing tests - if we want to make each test as independent as possible, then let\u0026rsquo;s leave them as they are. If we, however, are fine with having a common test setup code, then we could change the tests as follows -\n@ExtendWith(MockitoExtension.class) @DisplayName(\u0026#34;Given account does not exist When user adds a new account\u0026#34;) class AddNewAccountServiceTest { private static final String ACCOUNT_NAME = \u0026#34;test account\u0026#34;; private static final String INITIAL_BALANCE = \u0026#34;56.0\u0026#34;; @Mock private SaveAccountPort saveAccountPort; @Captor private ArgumentCaptor\u0026lt;Account\u0026gt; accountArgumentCaptor; @BeforeEach void setup() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); accountService.addNewAccount(ACCOUNT_NAME, INITIAL_BALANCE); } @Test @DisplayName(\u0026#34;Then added account has the given name\u0026#34;) void accountAddedWithGivenName() { BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); BDDAssertions.then(accountArgumentCaptor.getValue().getName()).isEqualTo(ACCOUNT_NAME); } @Test @DisplayName(\u0026#34;Then added account has the given initial balance\u0026#34;) void accountAddedWithGivenInitialBalance() { BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); BDDAssertions.then(accountArgumentCaptor.getValue().getBalance()) .isEqualTo(new BigDecimal(INITIAL_BALANCE)); } } Notice that we have also extracted the common part of the @DisplayName and put this on top of the test class. If we are not comfortable doing this, we could also leave them as they are.\nSince we have more than one passing tests, from now on every time we make a failing test pass we will stop for a moment, take a look at our implementation, and will try to improve it. To summarise, our implementation process will now consist of the following steps -\nAdd a failing test while making sure existing tests keep passing Make the failing test pass Pause for a moment and try to improve the implementation (both the code and the tests) Moving on, we now need to store user ids with the created account. Following our method we will first write a failing test to capture this and then add the minimal amount of code needed to make the failing test pass. This is how the implementation looks like once the failing test starts to pass -\n@ExtendWith(MockitoExtension.class) @DisplayName(\u0026#34;Given account does not exist When user adds a new account\u0026#34;) class AddNewAccountServiceTest { private static final String ACCOUNT_NAME = \u0026#34;test account\u0026#34;; private static final String INITIAL_BALANCE = \u0026#34;56.0\u0026#34;; private static final String USER_ID = \u0026#34;some id\u0026#34;; private Account savedAccount; @BeforeEach void setup() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); accountService.addNewAccount(ACCOUNT_NAME, INITIAL_BALANCE, USER_ID); BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); savedAccount = accountArgumentCaptor.getValue(); } // Other tests..... @Test @DisplayName(\u0026#34;Then added account has user\u0026#39;s id\u0026#34;) void accountAddedWithUsersId() { BDDAssertions.then(accountArgumentCaptor.getValue().getUserId()).isEqualTo(USER_ID); } } @RequiredArgsConstructor class AddNewAccountService { private final SaveAccountPort saveAccountPort; void addNewAccount(String accountName, String initialBalance, String userId) { saveAccountPort.saveAccount(new Account(accountName, new BigDecimal(initialBalance), userId)); } } @AllArgsConstructor @Getter class Account { private String name; private BigDecimal balance; private String userId; } Since all the tests are now passing, it\u0026rsquo;s improvement time! Notice that the addNewAccount method accepts three argument already. As we introduce more and more account properties its argument list will also start to increase. We could introduce a parameter object to avoid that -\n@RequiredArgsConstructor class AddNewAccountService { private final SaveAccountPort saveAccountPort; void addNewAccount(AddNewAccountCommand command) { saveAccountPort.saveAccount( new Account( command.getAccountName(), new BigDecimal(command.getInitialBalance()), command.getUserId() ) ); } @Builder @Getter static class AddNewAccountCommand { private final String userId; private final String accountName; private final String initialBalance; } } @ExtendWith(MockitoExtension.class) @DisplayName(\u0026#34;Given account does not exist When user adds a new account\u0026#34;) class AddNewAccountServiceTest { // Fields..... @BeforeEach void setup() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); AddNewAccountCommand command = AddNewAccountCommand.builder() .accountName(ACCOUNT_NAME) .initialBalance(INITIAL_BALANCE) .userId(USER_ID) .build(); accountService.addNewAccount(command); BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); savedAccount = accountArgumentCaptor.getValue(); } // Remaining Tests..... } If I now run the tests in my IDEA, this is what I see -\nWhen we try to read the test descriptions in this view we can already get a good overview of the Add New Account use case and the way it works.\nRight, let\u0026rsquo;s move on the the second scenario of our use case, which is a validation rule -\nGiven account does not exist When user adds a new account with negative initial balance Then add new account fails\nLet\u0026rsquo;s write a new test which tries to capture this -\n@ExtendWith(MockitoExtension.class) @DisplayName(\u0026#34;Given account does not exist When user adds a new account\u0026#34;) class AddNewAccountServiceTest { // Other tests @Test @DisplayName(\u0026#34;Given account does not exist When user adds a new account with negative initial balance Then add new account fails\u0026#34;) void addNewAccountFailsWithNegativeInitialBalance() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); AddNewAccountCommand command = AddNewAccountCommand.builder().initialBalance(\u0026#34;-56.0\u0026#34;).build(); accountService.addNewAccount(command); BDDMockito.then(saveAccountPort).shouldHaveNoInteractions(); } } There are several ways we can implement validations in our service. We could throw an exception detailing the validation failures, or we could return an error object which would contain the error details. For this example we will throw exceptions if validation fails -\n@Test @DisplayName(\u0026#34;Given account does not exist When user adds a new account with negative initial balance Then add new account fails\u0026#34;) void addNewAccountFailsWithNegativeInitialBalance() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); AddNewAccountCommand command = AddNewAccountCommand.builder().initialBalance(\u0026#34;-56.0\u0026#34;).build(); assertThatExceptionOfType(IllegalArgumentException.class) .isThrownBy(() -\u0026gt; accountService.addNewAccount(command)); BDDMockito.then(saveAccountPort).shouldHaveNoInteractions(); } This test verifies that an exception is thrown when the addNewAccount method is invoked with a negative balance. It also ensures that in such cases our code does not invoke any method of the SaveAccountPort. Before we can start modifying our service to make this test pass, we have to refactor our test setup code a bit. This is because during one of our previous refactoring we moved our common test setup code into a single method which now runs before each test -\n@BeforeEach void setup() { AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); AddNewAccountCommand command = AddNewAccountCommand.builder() .accountName(ACCOUNT_NAME) .initialBalance(INITIAL_BALANCE) .userId(USER_ID) .build(); accountService.addNewAccount(command); BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); savedAccount = accountArgumentCaptor.getValue(); } This setup code is now in direct conflict with the new test that we\u0026rsquo;ve just added - before each test it will always invoke the addNewAccount method with a valid command object, resulting in an invocation of the saveAccount method of the SaveAccountPort, causing our new test to fail.\nIn order to fix this, we will create a nested class within our test class where we will move our existing setup code and the passing tests -\n@ExtendWith(MockitoExtension.class) @DisplayName(\u0026#34;Given account does not exist\u0026#34;) class AddNewAccountServiceTest { @Mock private SaveAccountPort saveAccountPort; private AddNewAccountService accountService; @BeforeEach void setUp() { accountService = new AddNewAccountService(saveAccountPort); } @Nested @DisplayName(\u0026#34;When user adds a new account\u0026#34;) class WhenUserAddsANewAccount { private static final String ACCOUNT_NAME = \u0026#34;test account\u0026#34;; private static final String INITIAL_BALANCE = \u0026#34;56.0\u0026#34;; private static final String USER_ID = \u0026#34;some id\u0026#34;; private Account savedAccount; @Captor private ArgumentCaptor\u0026lt;Account\u0026gt; accountArgumentCaptor; @BeforeEach void setUp() { AddNewAccountCommand command = AddNewAccountCommand.builder() .accountName(ACCOUNT_NAME) .initialBalance(INITIAL_BALANCE) .userId(USER_ID) .build(); accountService.addNewAccount(command); BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); savedAccount = accountArgumentCaptor.getValue(); } @Test @DisplayName(\u0026#34;Then added account has the given name\u0026#34;) void accountAddedWithGivenName() { BDDAssertions.then(savedAccount.getName()).isEqualTo(ACCOUNT_NAME); } @Test @DisplayName(\u0026#34;Then added account has the given initial balance\u0026#34;) void accountAddedWithGivenInitialBalance() { BDDAssertions.then(savedAccount.getBalance()).isEqualTo(new BigDecimal(INITIAL_BALANCE)); } @Test @DisplayName(\u0026#34;Then added account has user\u0026#39;s id\u0026#34;) void accountAddedWithUsersId() { BDDAssertions.then(accountArgumentCaptor.getValue().getUserId()).isEqualTo(USER_ID); } } @Test @DisplayName(\u0026#34;When user adds a new account with negative initial balance Then add new account fails\u0026#34;) void addNewAccountFailsWithNegativeInitialBalance() { AddNewAccountCommand command = AddNewAccountCommand.builder() .initialBalance(\u0026#34;-56.0\u0026#34;) .build(); assertThatExceptionOfType(IllegalArgumentException.class) .isThrownBy(() -\u0026gt; accountService.addNewAccount(command)); BDDMockito.then(saveAccountPort).shouldHaveNoInteractions(); } } Here are the refactoring steps that we took -\nWe created an inner class and then marked the inner class with JUnit 5\u0026rsquo;s @Nested annotation. We broke down the @DisplayName label of the outermost test class and moved the \u0026ldquo;When user adds a new account\u0026rdquo; part to the newly introduced inner class. The reason we did this is because this inner class will contain the group of tests that will verify/validate behaviours related to a valid account creation scenario. We moved related setup code and fields/constants into this inner class. We have removed the \u0026ldquo;Given account does not exist\u0026rdquo; part from our new test. This is because the @DisplayName on the outermost test class already includes this, hence no point including it here again. This is how the tests now look like when I run them in my IntelliJ IDEA -\nAs we can see from the screenshot, our test labels are also grouped and indented nicely following the structure that we created in our test code. Let\u0026rsquo;s modify our service now to make the failing test pass -\nvoid addNewAccount(AddNewAccountCommand command) { BigDecimal initialBalance = new BigDecimal(command.getInitialBalance()); if (initialBalance.compareTo(BigDecimal.ZERO) \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;Initial balance of an account cannot be negative\u0026#34;); } saveAccountPort.saveAccount( new Account( command.getAccountName(), initialBalance, command.getUserId() ) ); } With that all of our tests start passing again. Next step is to look for ways to improve the existing implementation if possible. If not, then we will move on to the implementation of the final scenario which is also a validation rule -\nGiven account with the same name exists When user adds a new account Then add new account fails\nAs always, let\u0026rsquo;s write a test to capture this -\n@Test @DisplayName(\u0026#34;Given account with the same name exists When user adds a new account Then add new account fails\u0026#34;) void addNewAccountFailsForDuplicateAccounts() { AddNewAccountCommand command = AddNewAccountCommand.builder() .accountName(\u0026#34;existing name\u0026#34;) .build(); AddNewAccountService accountService = new AddNewAccountService(saveAccountPort); assertThatExceptionOfType(IllegalArgumentException.class) .isThrownBy(() -\u0026gt; accountService.addNewAccount(command)); BDDMockito.then(saveAccountPort).shouldHaveNoInteractions(); } First thing we have to figure out now is to how to find an existing account. Since this will involve querying our persistent data store, we will introduce an interface -\npublic interface FindAccountPort { Account findAccountByName(String accountName); } and inject it into our AddNewAccountService -\n@RequiredArgsConstructor class AddNewAccountService { private final SaveAccountPort saveAccountPort; private final FindAccountPort findAccountPort; // Rest of the code } and modify our test -\n@Test @DisplayName(\u0026#34;Given account with the same name exists When user adds a new account Then add new account fails\u0026#34;) void addNewAccountFailsForDuplicateAccounts() { String existingAccountName = \u0026#34;existing name\u0026#34;; AddNewAccountCommand command = AddNewAccountCommand.builder() .initialBalance(\u0026#34;0\u0026#34;) .accountName(existingAccountName) .build(); given(findAccountPort.findAccountByName(existingAccountName)).willReturn(mock(Account.class)); AddNewAccountService accountService = new AddNewAccountService(saveAccountPort, findAccountPort); assertThatExceptionOfType(IllegalArgumentException.class) .isThrownBy(() -\u0026gt; accountService.addNewAccount(command)); BDDMockito.then(saveAccountPort).shouldHaveNoInteractions(); } The last change to our AddNewAccountService will also require changes to our existing tests, mainly the place where we were instantiating an instance of that class. We will, however, change a bit more than that -\n@ExtendWith(MockitoExtension.class) class AddNewAccountServiceTest { @Mock private SaveAccountPort saveAccountPort; @Mock private FindAccountPort findAccountPort; @Nested @DisplayName(\u0026#34;Given account does not exist\u0026#34;) class AccountDoesNotExist { private AddNewAccountService accountService; @BeforeEach void setUp() { accountService = new AddNewAccountService(saveAccountPort, findAccountPort); } @Nested @DisplayName(\u0026#34;When user adds a new account\u0026#34;) class WhenUserAddsANewAccount { private static final String ACCOUNT_NAME = \u0026#34;test account\u0026#34;; private static final String INITIAL_BALANCE = \u0026#34;56.0\u0026#34;; private static final String USER_ID = \u0026#34;some id\u0026#34;; private Account savedAccount; @Captor private ArgumentCaptor\u0026lt;Account\u0026gt; accountArgumentCaptor; @BeforeEach void setUp() { AddNewAccountCommand command = AddNewAccountCommand.builder() .accountName(ACCOUNT_NAME) .initialBalance(INITIAL_BALANCE) .userId(USER_ID) .build(); accountService.addNewAccount(command); BDDMockito.then(saveAccountPort).should().saveAccount(accountArgumentCaptor.capture()); savedAccount = accountArgumentCaptor.getValue(); } @Test @DisplayName(\u0026#34;Then added account has the given name\u0026#34;) void accountAddedWithGivenName() { BDDAssertions.then(savedAccount.getName()).isEqualTo(ACCOUNT_NAME); } @Test @DisplayName(\u0026#34;Then added account has the given initial balance\u0026#34;) void accountAddedWithGivenInitialBalance() { BDDAssertions.then(savedAccount.getBalance()).isEqualTo(new BigDecimal(INITIAL_BALANCE)); } @Test @DisplayName(\u0026#34;Then added account has user\u0026#39;s id\u0026#34;) void accountAddedWithUsersId() { BDDAssertions.then(accountArgumentCaptor.getValue().getUserId()).isEqualTo(USER_ID); } } @Test @DisplayName(\u0026#34;When user adds a new account with negative initial balance Then add new account fails\u0026#34;) void addNewAccountFailsWithNegativeInitialBalance() { AddNewAccountCommand command = AddNewAccountCommand.builder() .initialBalance(\u0026#34;-56.0\u0026#34;) .build(); assertThatExceptionOfType(IllegalArgumentException.class) .isThrownBy(() -\u0026gt; accountService.addNewAccount(command)); BDDMockito.then(saveAccountPort).shouldHaveNoInteractions(); } } @Test @DisplayName(\u0026#34;Given account with the same name exists When user adds a new account Then add new account fails\u0026#34;) void addNewAccountFailsForDuplicateAccounts() { String existingAccountName = \u0026#34;existing name\u0026#34;; AddNewAccountCommand command = AddNewAccountCommand.builder() .initialBalance(\u0026#34;0\u0026#34;) .accountName(existingAccountName) .build(); given(findAccountPort.findAccountByName(existingAccountName)).willReturn(mock(Account.class)); AddNewAccountService accountService = new AddNewAccountService(saveAccountPort, findAccountPort); assertThatExceptionOfType(IllegalArgumentException.class) .isThrownBy(() -\u0026gt; accountService.addNewAccount(command)); BDDMockito.then(saveAccountPort).shouldHaveNoInteractions(); } } Here\u0026rsquo;s what we did -\nWe created another inner class, marked it as @Nested, and moved our existing passing tests into this. This group of tests test the behaviour of adding a new account when no account with the given name already exists. We have moved our test set up code into the newly introduced inner class as they are also related to the \u0026ldquo;no account with the given name already exists\u0026rdquo; case. For the same reason as above, we have also moved our @DisplayName annotation from the top level test class to the newly introduced inner class.\nAfter our refactoring we quickly run our tests to see if everything is working as expected (failing test failing, passing tests passing), and then move on to modify our service -\n@RequiredArgsConstructor class AddNewAccountService { private final SaveAccountPort saveAccountPort; private final FindAccountPort findAccountPort; void addNewAccount(AddNewAccountCommand command) { BigDecimal initialBalance = new BigDecimal(command.getInitialBalance()); if (initialBalance.compareTo(BigDecimal.ZERO) \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;Initial balance of an account cannot be negative\u0026#34;); } if (findAccountPort.findAccountByName(command.getAccountName()) != null) { throw new IllegalArgumentException(\u0026#34;An account with given name already exists\u0026#34;); } saveAccountPort.saveAccount( new Account( command.getAccountName(), initialBalance, command.getUserId() ) ); } @Builder @Getter static class AddNewAccountCommand { private final String userId; private final String accountName; private final String initialBalance; } } All of our tests should now be green.\nSince our use case implementation is now complete, we will look at our implementation for one last time and see if we can improve anything. If not, our use case implementation is now complete!\nTo summarise, this is what we did throughout this article -\nWe have written down a use case that we would like to implement We have added a failing test, labelling it with a human-readable name We have added the minimal amount of code needed to make the failing test pass As soon as we had more than one passing tests, after we made each failing test pass, we looked at our implementation and tried to improve it When writing the tests we tried writing them in such a way so that our use case specifications are reflected in the test code. For this we have used - The @DisplayName annotation to assign human-readable names to our tests Used @Nested to group related tests in a hierarchical structure, reflecting our use case setup Used BDD-driven API from Mockito and AssertJ to verify the expected behaviours When should we follow this style of writing automated tests? The answer to this question is the same as every other usage questions in Software Engineering - it depends. I personally prefer this style when I am working with an application which has complex business/domain rules, which is intended to be maintained over a long period, for which a close collaboration with the business is required, and many other factors (i.e., application architecture, team adoption etc.).\nAs always, the full working example has been pushed to Github.\nUntil next time!\n","permalink":"https://sayembd.github.io/posts/writing-executable-specifications-with-junit5-mockito-and-assertj/","summary":"Executable Specifications are tests that can also serve as design specifications. They enable technical and business teams to get on the same page by enabling the use of a common language (in DDD-world this is also known as Ubiquitous Language). They function as documentations for the future maintainers of the code. In this article we will see an opinionated way of writing automated tests which could also function as Executable Specifications.","title":"Writing Executable Specifications With Junit5 Mockito and AssertJ"},{"content":"Introduction The Fork/Join framework is a framework to solve a problem using a concurrent divide-and-conquer approach. They were introduced to complement the existing concurrency API. Before their introduction, the existing ExecutorService implementations were the popular choice to run asynchronous tasks, but they work best when the tasks are homogenous and independent. Running dependent tasks and combining their results using those implementations were not easy. With the introduction of the Fork/Join framework, an attempt was made to address this shortcoming. In this post, we will take a brief look at the API and solve a couple of simple problems to understand how they work.\nSolving a non-blocking task Let\u0026rsquo;s jump directly into code. Let\u0026rsquo;s create a task which would return the sum of all elements of a List. The following steps represent our algorithm in pseudo-code:\nFind the middle index of the list Divide the list in the middle Recursively create a new task which will compute the sum of the left part Recursively create a new task which will compute the sum of the right part Add the result of the left sum, the middle element, and the right sum Here is the code -\n@Slf4j public class ListSummer extends RecursiveTask\u0026lt;Integer\u0026gt; { private final List\u0026lt;Integer\u0026gt; listToSum; ListSummer(List\u0026lt;Integer\u0026gt; listToSum) { this.listToSum = listToSum; } @Override protected Integer compute() { if (listToSum.isEmpty()) { log.info(\u0026#34;Found empty list, sum is 0\u0026#34;); return 0; } int middleIndex = listToSum.size() / 2; log.info(\u0026#34;List {}, middle Index: {}\u0026#34;, listToSum, middleIndex); List\u0026lt;Integer\u0026gt; leftSublist = listToSum.subList(0, middleIndex); List\u0026lt;Integer\u0026gt; rightSublist = listToSum.subList(middleIndex + 1, listToSum.size()); ListSummer leftSummer = new ListSummer(leftSublist); ListSummer rightSummer = new ListSummer(rightSublist); leftSummer.fork(); rightSummer.fork(); Integer leftSum = leftSummer.join(); Integer rightSum = rightSummer.join(); int total = leftSum + listToSum.get(middleIndex) + rightSum; log.info(\u0026#34;Left sum is {}, right sum is {}, total is {}\u0026#34;, leftSum, rightSum, total); return total; } } Firstly, we extend the RecursiveTask subtype of the ForkJoinTask. This is the type to extend from when we expect our concurrent task to return a result. When a task does not return a result but only perform an effect, we extend the RecursiveAction subtype. For most of the practical tasks that we solve, these two subtypes are sufficient.\nSecondly, both RecursiveTask and RecursiveAction define an abstract compute method. This is where we put our computation.\nThirdly, inside our compute method, we check the size of the list that is passed through the constructor. If it is empty, we already know the result of the sum which is zero, and we return immediately. Otherwise, we divide our lists into two sublists and create two instances of our ListSummer type. We then call the fork() method (defined in ForkJoinTask) on these two instances -\nleftSummer.fork(); rightSummer.fork(); Which cause these tasks to be scheduled for asynchronous execution, the exact mechanism which is used for this purpose will be explained later in this post.\nAfter that, we invoke the join() method (also defined in ForkJoinTask) to wait for the result of these two parts -\nInteger leftSum = leftSummer.join(); Integer rightSum = rightSummer.join(); Which are then summed with the middle element of the list to get the final result.\nPlenty of log messages have been added to make the example easier to understand. However, when we process a list containing thousands of entries, it might not be a good idea to have this detailed logging, especially logging the entire list.\nThat\u0026rsquo;s pretty much it. Let\u0026rsquo;s create a test class now for a test run -\npublic class ListSummerTest { @Test public void shouldSumEmptyList() { ListSummer summer = new ListSummer(List.of()); ForkJoinPool forkJoinPool = new ForkJoinPool(); forkJoinPool.submit(summer); int result = summer.join(); assertThat(result).isZero(); } @Test public void shouldSumListWithOneElement() { ListSummer summer = new ListSummer(List.of(5)); ForkJoinPool forkJoinPool = new ForkJoinPool(); forkJoinPool.submit(summer); int result = summer.join(); assertThat(result).isEqualTo(5); } @Test public void shouldSumListWithMultipleElements() { ListSummer summer = new ListSummer(List.of( 1, 2, 3, 4, 5, 6, 7, 8, 9 )); ForkJoinPool forkJoinPool = new ForkJoinPool(); forkJoinPool.submit(summer); int result = summer.join(); assertThat(result).isEqualTo(45); } } In the test, we create an instance of the ForkJoinPool. A ForkJoinPool is a unique ExecutorService implementation for running ForkJoinTasks. It employs a special algorithm known as the work-stealing algorithm. Contrary to the other ExecutorService implementations where there is only a single queue holding all the tasks to be executed, in a work-stealing implementation, each worker thread gets its work queue. Each thread starts executing tasks from their queue. When we detect that a ForkJoinTask can be broken down into multiple smaller subtasks, we do break them into smaller tasks, and then we invoke the fork() method on those tasks. This invocation causes the subtasks to be pushed into the executing thread\u0026rsquo;s queue. During the execution, when one thread exhausts its queue/has no tasks to execute, it can \u0026ldquo;steal\u0026rdquo; tasks from other thread\u0026rsquo;s queue (hence the name \u0026ldquo;work-stealing\u0026rdquo;). This stealing behaviour is what results in a better throughput than using any other ExecutorService implementations.\nEarlier, when we invoked fork() on our leftSummer and rightSummer task instances, they got pushed into the work queue of the executing thread, after which they were \u0026ldquo;stolen\u0026rdquo; by other active threads in the pool (and so on) since they did not have anything else to do at that point.\nPretty cool, right?\nSolving a blocking task The problem we solved just now is non-blocking in nature. If we want to solve a problem which does some blocking operation, then to have a better throughput we will need to change our strategy.\nLet\u0026rsquo;s examine this with another example. Let\u0026rsquo;s say we want to create a very simple web crawler. This crawler will receive a list of HTTP links, execute GET requests to fetch the response bodies, and then calculate the response length. Here is the code -\n@Slf4j public class ResponseLengthCalculator extends RecursiveTask\u0026lt;Map\u0026lt;String, Integer\u0026gt;\u0026gt; { private final List\u0026lt;String\u0026gt; links; ResponseLengthCalculator(List\u0026lt;String\u0026gt; links) { this.links = links; } @Override protected Map\u0026lt;String, Integer\u0026gt; compute() { if (links.isEmpty()) { log.info(\u0026#34;No more links to fetch\u0026#34;); return Collections.emptyMap(); } int middle = links.size() / 2; log.info(\u0026#34;Middle index: {}\u0026#34;, links, middle); ResponseLengthCalculator leftPartition = new ResponseLengthCalculator(links.subList(0, middle)); ResponseLengthCalculator rightPartition = new ResponseLengthCalculator(links.subList(middle + 1, links.size())); log.info(\u0026#34;Forking left partition\u0026#34;); leftPartition.fork(); log.info(\u0026#34;Left partition forked, now forking right partition\u0026#34;); rightPartition.fork(); log.info(\u0026#34;Right partition forked\u0026#34;); String middleLink = links.get(middle); HttpRequester httpRequester = new HttpRequester(middleLink); String response; try { log.info(\u0026#34;Calling managedBlock for {}\u0026#34;, middleLink); ForkJoinPool.managedBlock(httpRequester); response = httpRequester.response; } catch (InterruptedException ex) { log.error(\u0026#34;Error occurred while trying to implement blocking link fetcher\u0026#34;, ex); response = \u0026#34;\u0026#34;; } Map\u0026lt;String, Integer\u0026gt; responseMap = new HashMap\u0026lt;\u0026gt;(links.size()); Map\u0026lt;String, Integer\u0026gt; leftLinks = leftPartition.join(); responseMap.putAll(leftLinks); responseMap.put(middleLink, response.length()); Map\u0026lt;String, Integer\u0026gt; rightLinks = rightPartition.join(); responseMap.putAll(rightLinks); log.info(\u0026#34;Left map {}, middle length {}, right map {}\u0026#34;, leftLinks, response.length(), rightLinks); return responseMap; } private static class HttpRequester implements ForkJoinPool.ManagedBlocker { private final String link; private String response; private HttpRequester(String link) { this.link = link; } @Override public boolean block() { HttpGet headRequest = new HttpGet(link); CloseableHttpClient client = HttpClientBuilder .create() .disableRedirectHandling() .build(); log.info(\u0026#34;Executing blocking request for {}\u0026#34;, link); try (client; CloseableHttpResponse response = client.execute(headRequest)) { log.info(\u0026#34;HTTP request for link {} has been executed\u0026#34;, link); this.response = EntityUtils.toString(response.getEntity()); } catch (IOException e) { log.error(\u0026#34;Error while trying to fetch response from link {}: {}\u0026#34;, link, e.getMessage()); this.response = \u0026#34;\u0026#34;; } return true; } @Override public boolean isReleasable() { return false; } } } We create an implementation of the ForkJoinPool.ManagedBlocker where we put the blocking HTTP call. This interface defines two methods - block() and isReleasable(). The block() method is where we put our blocking call. After we are done with our blocking operation, we return true indicating that no further blocking is necessary. We return false from the isReleasable() implementation to indicate to a fork-join worker thread that the block() method implementation is potentially blocking in nature. The isReleasable() implementation will be invoked by a fork-join worker thread first before it invokes the block() method. Finally, we submit our HttpRequester instance to our pool by invoking ForkJoinPool.managedBlock() static method. After that our blocking task will start executing. When it blocks on the HTTP request, the ForkJoinPool.managedBlock() method will also arrange for a spare thread to be activated if necessary to ensure sufficient parallelism.\nLet\u0026rsquo;s take this implementation for a test drive then! Here\u0026rsquo;s the code -\npublic class ResponseLengthCalculatorTest { @Test public void shouldReturnEmptyMapForEmptyList() { ResponseLengthCalculator responseLengthCalculator = new ResponseLengthCalculator(Collections.emptyList()); ForkJoinPool pool = new ForkJoinPool(); pool.submit(responseLengthCalculator); Map\u0026lt;String, Integer\u0026gt; result = responseLengthCalculator.join(); assertThat(result).isEmpty(); } @Test public void shouldHandle200Ok() { ResponseLengthCalculator responseLengthCalculator = new ResponseLengthCalculator(List.of( \u0026#34;http://httpstat.us/200\u0026#34; )); ForkJoinPool pool = new ForkJoinPool(); pool.submit(responseLengthCalculator); Map\u0026lt;String, Integer\u0026gt; result = responseLengthCalculator.join(); assertThat(result) .hasSize(1) .containsKeys(\u0026#34;http://httpstat.us/200\u0026#34;) .containsValue(0); } @Test public void shouldFetchResponseForDifferentResponseStatus() { ResponseLengthCalculator responseLengthCalculator = new ResponseLengthCalculator(List.of( \u0026#34;http://httpstat.us/200\u0026#34;, \u0026#34;http://httpstat.us/302\u0026#34;, \u0026#34;http://httpstat.us/404\u0026#34;, \u0026#34;http://httpstat.us/502\u0026#34; )); ForkJoinPool pool = new ForkJoinPool(); pool.submit(responseLengthCalculator); Map\u0026lt;String, Integer\u0026gt; result = responseLengthCalculator.join(); assertThat(result) .hasSize(4); } } That\u0026rsquo;s it for today, folks! As always, any feedback/improvement suggestions/comments are highly appreciated!\nAll the examples discussed here can be found on Github.\nA big shout out to the awesome http://httpstat.us service, it was quite helpful for developing the simple tests.\n","permalink":"https://sayembd.github.io/posts/a-brief-overview-of-the-fork-join-framework-in-java/","summary":"Introduction The Fork/Join framework is a framework to solve a problem using a concurrent divide-and-conquer approach. They were introduced to complement the existing concurrency API. Before their introduction, the existing ExecutorService implementations were the popular choice to run asynchronous tasks, but they work best when the tasks are homogenous and independent. Running dependent tasks and combining their results using those implementations were not easy. With the introduction of the Fork/Join framework, an attempt was made to address this shortcoming.","title":"A Brief Overview of the Fork Join Framework in Java"}]